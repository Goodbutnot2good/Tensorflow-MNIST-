{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\max\\desktop\\env\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "(x_set, y_set), (x_test, y_test) = mnist.load_data()\n",
    "x_set = x_set/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Given we will not be having class next week and I cannot reasonably expect you to do work for which we will not have lectured; this weeks sprint will be broken up into two smaller pieces as was lossely voted on in class, with this being part 1.\n",
    "\n",
    "For this sprint you will be doing a process called K-Fold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "In class you were briefly introduced to Keras, which is a high level machine learning library that can be used to create everything from an introductory model such as what you will be building to very complex models used in industry every day to handle everything from chat bots to object detection and more.\n",
    "\n",
    "### Section 1\n",
    "\n",
    "In the last sprint you did some exploration that helped you understand the dataset and what was in it, this time you are going to prepare it for training. \n",
    "\n",
    "Professor Memon had talked about in his lecture taking your data and properly holding back some of it so that later you could use it to validate if your model was working or not.\n",
    "\n",
    "For this section you will be responsible for implementing in python an algorithm called K-Fold\n",
    "\n",
    "This will be worth **40** points of the sprint\n",
    "\n",
    "\n",
    "### Section 2\n",
    "\n",
    "With K = 5 for the number of folds you will do the below:\n",
    "\n",
    "Now that you have properly segmented your data you will have to train K-1 models and validate them. The code for the model has already been implemented, you do not need to worry about that.\n",
    "\n",
    "The general procedure is:\n",
    "    1. Split your dataset into K even sets of data using the k-fold algorithm.\n",
    "    2. Train a model on set K=0\n",
    "    3. Validate the model on set K=1\n",
    "    4. Repeat for K+1 and K+2\n",
    "    \n",
    "**Note:** Training the models will take some time depending on your computer, each model will be saved so after you are sure this part is working you should only have to do it once. If you mess something up you can delete the model files and start again.\n",
    "    \n",
    "This will be worth **40** points of the sprint\n",
    "\n",
    "### Section 3\n",
    "Provide a few sentences about common pitfalls of k-fold-cross validation and training models with it.\n",
    "\n",
    "This will be worth **20** points of the sprint\n",
    "\n",
    "### Extra credit\n",
    "\n",
    "There are very many other validation methods for constructing machine learning models. Find one and implement it.\n",
    "This is worth **20** extra credit points for the sprint.\n",
    "\n",
    "\n",
    "#### Note:\n",
    "Before you begin, you can use the same virtual environments you created last week, but you must pip install h5py into them. h5py is a file format library that will be used to save the trained models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within k fold split function\n",
      "x_set type is<class 'numpy.ndarray'>\n",
      "y_set type is<class 'numpy.ndarray'>\n",
      "X set is 60000\n",
      "Y set is 60000\n",
      "len of x final list 5\n",
      "len of y final list 5\n",
      "x_final_type <class 'list'>\n",
      "y_final_type <class 'list'>\n",
      "x_final_type[0] <class 'numpy.ndarray'>\n",
      "y_final_type[0] <class 'list'>\n",
      "length of y final[0] 12000\n",
      "type of y final[0] <class 'list'>\n",
      "x_final_type[0] shape (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "def k_fold_split(x_set, y_set, folds=1):\n",
    "    '''\n",
    "    Inputs: The x_set data from mnist, the y_set labels from mnist\n",
    "    Expected Output: The shuffled and K split datasets\n",
    "    '''\n",
    "    print(\"within k fold split function\")\n",
    "    np.random.seed(1)\n",
    "    fold_size = int(len(x_set) / folds)\n",
    "    \n",
    "    x_set_temp = np.reshape(x_set, (60000, 784))\n",
    "    \n",
    "    print(\"x_set type is\" + str(type(x_set)))\n",
    "    print(\"y_set type is\" + str(type(y_set)))\n",
    "    print(\"X set is \" + str(len(x_set)))\n",
    "    print(\"Y set is \" + str(len(y_set)))\n",
    "    \n",
    "    combined_list = [(x_set_temp[i], y_set[i]) for i in range(len(x_set))]\n",
    "    np.random.shuffle(combined_list)\n",
    "    \n",
    "    five_lists = [combined_list[i::folds] for i in range(folds)]\n",
    "    \n",
    "    #one list that holds 5 lists\n",
    "    x_final_list = [tup[0] for lst in five_lists for tup in lst]\n",
    "    x_final_list = [x_final_list[i::folds] for i in range(folds)]\n",
    "    for i in range(len(x_final_list)):\n",
    "        x_final_list[i] = np.array(x_final_list[i])\n",
    "    \n",
    "    y_final_list = [tup[1] for lst in five_lists for tup in lst]\n",
    "    y_final_list = [y_final_list[i::folds] for i in range(folds)]\n",
    "    #each item is a numpy array with shape (12000, 784)\n",
    "    #for i in range(len(y_final_list)):\n",
    "        #y_final_list[i] = np.array(y_final_list[i])\n",
    "    \n",
    "    print(\"len of x final list \" + str(len(x_final_list)))\n",
    "    print(\"len of y final list \" + str(len(y_final_list)))\n",
    "    print(\"x_final_type \" + str(type(x_final_list)))\n",
    "    print(\"y_final_type \" + str(type(y_final_list)))\n",
    "    print(\"x_final_type[0] \" + str(type(x_final_list[0])))\n",
    "    print(\"y_final_type[0] \" + str(type(y_final_list[0])))\n",
    "    print(\"length of y final[0] \" + str(len(y_final_list[0])))\n",
    "    print(\"type of y final[0] \" + str(type(y_final_list[0])))\n",
    "    print(\"x_final_type[0] shape \" + str(x_final_list[0].shape))\n",
    "    #print(\"y_final_type shape\" + str(y_final_list[0].shape))\n",
    "    \n",
    "    return x_final_list, y_final_list\n",
    "    \n",
    "x_folds, y_folds = k_fold_split(x_set, y_set, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Epochs are the number of times the dataset will be iterated over, a good number is 20\n",
    "def train_model(model, train_dataset, validation_dataset, epochs, name):\n",
    "    x_set, y_set = train_dataset\n",
    "    model.fit(x_set, y_set, epochs=epochs, batch_size=128, validation_data=validation_dataset)\n",
    "    model.save(f'./{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: Neural Networks can't just handle the lables as they are, they need --categorical-- data\n",
    "#Note: You must submit the trained models along with the notebook for full credit\n",
    "def train_validate_k(x_folds , y_folds, num_folds):\n",
    "    '''\n",
    "        Inputs: x_folds, the x folds returned from the k_fold algorithm above, \n",
    "        y_folds the y folds returned from the k_fold algorithm above\n",
    "        num_folds, the number of folds used to make the x_folds and y_folds\n",
    "        Expected Output: Nothing, this function has no explicit output, \n",
    "        but there must be num_fold models trained and saved to disk\n",
    "    '''\n",
    "    print(\"The length of x folds is \" + str(len(x_folds)))\n",
    "    print(\"The length of y folds is \" + str(len(y_folds)))\n",
    "    print(\"the type of x folds is \" + str(type(x_folds)))\n",
    "    print(\"the type of y folds is \" + str(type(y_folds)))\n",
    "    print(\"the type of first x folds is \" + str(type(x_folds[0])))\n",
    "    print(\"the type of first y folds is \" + str(type(y_folds[0])))\n",
    "    \n",
    "    \n",
    "    x_dataset, y_dataset = k_fold_split(x_folds, y_folds, num_folds)\n",
    "    print(\"made it past dataset\")\n",
    "    print(\"length of x dataset\" + str(len(x_dataset)))\n",
    "    print(\"length of y dataset\" + str(len(y_dataset)))\n",
    "    for i in range(num_folds-1):\n",
    "        model = construct_model()\n",
    "        \n",
    "        x_train_dataset = x_dataset[i]\n",
    "        y_train_dataset = y_dataset[i]\n",
    "        train_dataset = (x_train_dataset, y_train_dataset)\n",
    "        \n",
    "        print(\"shape of y dataset is \" + str(np.array(y_dataset).shape))\n",
    "        x_validate_dataset = x_dataset[i+1]\n",
    "        y_validate_dataset = y_dataset[i+1]\n",
    "        validate_dataset = (x_validate_dataset, y_validate_dataset)\n",
    "        print(np.array(y_validate_dataset).shape)\n",
    "        y_train = to_categorical(train_dataset[1]).reshape((1, -1))\n",
    "        y_validate = to_categorical(validate_dataset[1]).reshape((1, -1))\n",
    "        print(to_categorical(train_dataset[1]).reshape(1, -1).shape)\n",
    "        print(to_categorical(validate_dataset[1]).reshape(1, -1).shape)\n",
    "        \n",
    "        print(\"type of x_train_dataset \" + str(type(x_train_dataset)))\n",
    "        print(\"x train dataset shape \" + str(x_train_dataset.shape))\n",
    "        print(\"x trai ndataset[0] shape\" + str(x_train_dataset[0].shape))\n",
    "        print(\"type of y_train_dataset \" + str(type(y_train_dataset)))\n",
    "        print(\"type of y_train_dataset \" + str(type(y_train_dataset)))\n",
    "        print(\"type of train_dataset \" + str(type(train_dataset)))\n",
    "        print(\"length of train dataset \" + str(len(train_dataset)))\n",
    "        print(y_train)\n",
    "        print(y_validate)\n",
    "        \n",
    "        train_model(model, (train_dataset[0], y_train), (validate_dataset[0], y_validate), 20, 'xy_tv')\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of x folds is 5\n",
      "The length of y folds is 5\n",
      "the type of x folds is <class 'list'>\n",
      "the type of y folds is <class 'list'>\n",
      "the type of first x folds is <class 'numpy.ndarray'>\n",
      "the type of first y folds is <class 'list'>\n",
      "within k fold split function\n",
      "x_set type is<class 'list'>\n",
      "y_set type is<class 'list'>\n",
      "X set is 5\n",
      "Y set is 5\n",
      "len of x final list 5\n",
      "len of y final list 5\n",
      "x_final_type <class 'list'>\n",
      "y_final_type <class 'list'>\n",
      "x_final_type[0] <class 'numpy.ndarray'>\n",
      "y_final_type[0] <class 'list'>\n",
      "length of y final[0] 1\n",
      "type of y final[0] <class 'list'>\n",
      "x_final_type[0] shape (1, 784)\n",
      "made it past dataset\n",
      "length of x dataset5\n",
      "length of y dataset5\n",
      "shape of y dataset is (5, 1, 12000)\n",
      "(1, 12000)\n",
      "(1, 120000)\n",
      "(1, 120000)\n",
      "type of x_train_dataset <class 'numpy.ndarray'>\n",
      "x train dataset shape (1, 784)\n",
      "x trai ndataset[0] shape(784,)\n",
      "type of y_train_dataset <class 'list'>\n",
      "type of y_train_dataset <class 'list'>\n",
      "type of train_dataset <class 'tuple'>\n",
      "length of train dataset 2\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_123 to have shape (10,) but got array with shape (120000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-afac3c75ef30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_validate_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-9ee0d1789349>\u001b[0m in \u001b[0;36mtrain_validate_k\u001b[1;34m(x_folds, y_folds, num_folds)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xy_tv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-b04f37a13cc2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, validation_dataset, epochs, name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mx_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./{name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\desktop\\env\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\max\\desktop\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\desktop\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mc:\\users\\max\\desktop\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_123 to have shape (10,) but got array with shape (120000,)"
     ]
    }
   ],
   "source": [
    "train_validate_k(x_folds, y_folds, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3, write a few sentences below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

