{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\max\\desktop\\env\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "(x_set, y_set), (x_test, y_test) = mnist.load_data()\n",
    "x_set = x_set/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Given we will not be having class next week and I cannot reasonably expect you to do work for which we will not have lectured; this weeks sprint will be broken up into two smaller pieces as was lossely voted on in class, with this being part 1.\n",
    "\n",
    "For this sprint you will be doing a process called K-Fold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "In class you were briefly introduced to Keras, which is a high level machine learning library that can be used to create everything from an introductory model such as what you will be building to very complex models used in industry every day to handle everything from chat bots to object detection and more.\n",
    "\n",
    "### Section 1\n",
    "\n",
    "In the last sprint you did some exploration that helped you understand the dataset and what was in it, this time you are going to prepare it for training. \n",
    "\n",
    "Professor Memon had talked about in his lecture taking your data and properly holding back some of it so that later you could use it to validate if your model was working or not.\n",
    "\n",
    "For this section you will be responsible for implementing in python an algorithm called K-Fold\n",
    "\n",
    "This will be worth **40** points of the sprint\n",
    "\n",
    "\n",
    "### Section 2\n",
    "\n",
    "With K = 5 for the number of folds you will do the below:\n",
    "\n",
    "Now that you have properly segmented your data you will have to train K-1 models and validate them. The code for the model has already been implemented, you do not need to worry about that.\n",
    "\n",
    "The general procedure is:\n",
    "    1. Split your dataset into K even sets of data using the k-fold algorithm.\n",
    "    2. Train a model on set K=0\n",
    "    3. Validate the model on set K=1\n",
    "    4. Repeat for K+1 and K+2\n",
    "    \n",
    "**Note:** Training the models will take some time depending on your computer, each model will be saved so after you are sure this part is working you should only have to do it once. If you mess something up you can delete the model files and start again.\n",
    "    \n",
    "This will be worth **40** points of the sprint\n",
    "\n",
    "### Section 3\n",
    "Provide a few sentences about common pitfalls of k-fold-cross validation and training models with it.\n",
    "\n",
    "This will be worth **20** points of the sprint\n",
    "\n",
    "### Extra credit\n",
    "\n",
    "There are very many other validation methods for constructing machine learning models. Find one and implement it.\n",
    "This is worth **20** extra credit points for the sprint.\n",
    "\n",
    "\n",
    "#### Note:\n",
    "Before you begin, you can use the same virtual environments you created last week, but you must pip install h5py into them. h5py is a file format library that will be used to save the trained models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within k fold split function\n",
      "x_set type is<class 'numpy.ndarray'>\n",
      "y_set type is<class 'numpy.ndarray'>\n",
      "X set is 60000\n",
      "Y set is 60000\n",
      "len of x final list 5\n",
      "len of y final list 5\n",
      "x_final_type <class 'list'>\n",
      "y_final_type <class 'list'>\n",
      "x_final_type[0] <class 'numpy.ndarray'>\n",
      "y_final_type[0] <class 'list'>\n",
      "length of y final[0] 12000\n",
      "type of y final[0] <class 'list'>\n",
      "x_final_type[0] shape (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "def k_fold_split(x_set, y_set, folds=1):\n",
    "    '''\n",
    "    Inputs: The x_set data from mnist, the y_set labels from mnist\n",
    "    Expected Output: The shuffled and K split datasets\n",
    "    '''\n",
    "    print(\"within k fold split function\")\n",
    "    np.random.seed(1)\n",
    "    fold_size = int(len(x_set) / folds)\n",
    "    \n",
    "    x_set_temp = np.reshape(x_set, (60000, 784))\n",
    "    \n",
    "    print(\"x_set type is\" + str(type(x_set)))\n",
    "    print(\"y_set type is\" + str(type(y_set)))\n",
    "    print(\"X set is \" + str(len(x_set)))\n",
    "    print(\"Y set is \" + str(len(y_set)))\n",
    "    \n",
    "    combined_list = [(x_set_temp[i], y_set[i]) for i in range(len(x_set))]\n",
    "    np.random.shuffle(combined_list)\n",
    "    \n",
    "    five_lists = [combined_list[i::folds] for i in range(folds)]\n",
    "    \n",
    "    #one list that holds 5 lists\n",
    "    x_final_list = [tup[0] for lst in five_lists for tup in lst]\n",
    "    x_final_list = [x_final_list[i::folds] for i in range(folds)]\n",
    "    for i in range(len(x_final_list)):\n",
    "        x_final_list[i] = np.array(x_final_list[i])\n",
    "    \n",
    "    y_final_list = [tup[1] for lst in five_lists for tup in lst]\n",
    "    y_final_list = [y_final_list[i::folds] for i in range(folds)]\n",
    "    #each item is a numpy array with shape (12000, 784)\n",
    "    #for i in range(len(y_final_list)):\n",
    "        #y_final_list[i] = np.array(y_final_list[i])\n",
    "    \n",
    "    print(\"len of x final list \" + str(len(x_final_list)))\n",
    "    print(\"len of y final list \" + str(len(y_final_list)))\n",
    "    print(\"x_final_type \" + str(type(x_final_list)))\n",
    "    print(\"y_final_type \" + str(type(y_final_list)))\n",
    "    print(\"x_final_type[0] \" + str(type(x_final_list[0])))\n",
    "    print(\"y_final_type[0] \" + str(type(y_final_list[0])))\n",
    "    print(\"length of y final[0] \" + str(len(y_final_list[0])))\n",
    "    print(\"type of y final[0] \" + str(type(y_final_list[0])))\n",
    "    print(\"x_final_type[0] shape \" + str(x_final_list[0].shape))\n",
    "    #print(\"y_final_type shape\" + str(y_final_list[0].shape))\n",
    "    \n",
    "    return x_final_list, y_final_list\n",
    "    \n",
    "x_folds, y_folds = k_fold_split(x_set, y_set, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Epochs are the number of times the dataset will be iterated over, a good number is 20\n",
    "def train_model(model, train_dataset, validation_dataset, epochs, name):\n",
    "    x_set, y_set = train_dataset\n",
    "    model.fit(x_set, y_set, epochs=epochs, batch_size=128, validation_data=validation_dataset)\n",
    "    model.save(f'./{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: Neural Networks can't just handle the lables as they are, they need --categorical-- data\n",
    "#Note: You must submit the trained models along with the notebook for full credit\n",
    "def train_validate_k(x_folds , y_folds, num_folds):\n",
    "    '''\n",
    "        Inputs: x_folds, the x folds returned from the k_fold algorithm above, \n",
    "        y_folds the y folds returned from the k_fold algorithm above\n",
    "        num_folds, the number of folds used to make the x_folds and y_folds\n",
    "        Expected Output: Nothing, this function has no explicit output, \n",
    "        but there must be num_fold models trained and saved to disk\n",
    "    '''\n",
    "    \n",
    "    #use 10 as the size or else you get \"expected dense to be array (10,) but got array (12,000,)\". Resize to account for this\n",
    "    size = 10\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        model = construct_model()\n",
    "    \n",
    "        #create the train dataset\n",
    "        x_train_dataset = np.reshape(x_folds[i], (-1, 784))\n",
    "        y_train_dataset = to_categorical(y_folds[i], num_classes=size)\n",
    "        train_dataset = (x_train_dataset, y_train_dataset)\n",
    "        \n",
    "        #make the next index be i+1 % num_folds to avoid index out of bounds error when doing test of 5 and validate at 0.\n",
    "        \n",
    "        #create the validate dataset\n",
    "        x_validate_dataset = np.reshape(x_folds[((i+1)%num_folds)], (-1, 784))\n",
    "        y_validate_dataset = to_categorical(y_folds[((i+1)%num_folds)], num_classes=size)\n",
    "        validate_dataset = (x_validate_dataset, y_validate_dataset)\n",
    "        \n",
    "        #train the model\n",
    "        train_model(model, train_dataset, validate_dataset, 20, 'Train_Model_'+str(i))\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 3s 235us/step - loss: 0.4949 - acc: 0.8433 - val_loss: 0.2709 - val_acc: 0.9158\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 2s 208us/step - loss: 0.2180 - acc: 0.9339 - val_loss: 0.1755 - val_acc: 0.9451\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.1487 - acc: 0.9525 - val_loss: 0.2071 - val_acc: 0.9360\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 3s 231us/step - loss: 0.1047 - acc: 0.9664 - val_loss: 0.1669 - val_acc: 0.9494\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 3s 243us/step - loss: 0.0781 - acc: 0.9757 - val_loss: 0.1570 - val_acc: 0.9543\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0603 - acc: 0.9796 - val_loss: 0.1283 - val_acc: 0.9627\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 3s 218us/step - loss: 0.0419 - acc: 0.9875 - val_loss: 0.1553 - val_acc: 0.9588\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.0332 - acc: 0.9892 - val_loss: 0.1749 - val_acc: 0.9566\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 218us/step - loss: 0.0282 - acc: 0.9906 - val_loss: 0.1507 - val_acc: 0.9613\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0247 - acc: 0.9919 - val_loss: 0.1691 - val_acc: 0.9616\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.1780 - val_acc: 0.9625\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0196 - acc: 0.9940 - val_loss: 0.1904 - val_acc: 0.9617\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0162 - acc: 0.9943 - val_loss: 0.1827 - val_acc: 0.9622\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.1929 - val_acc: 0.9630\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0155 - acc: 0.9951 - val_loss: 0.1745 - val_acc: 0.9666\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.2200 - val_acc: 0.9602\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 211us/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.1900 - val_acc: 0.9664\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 3s 213us/step - loss: 0.0092 - acc: 0.9968 - val_loss: 0.1902 - val_acc: 0.9663\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.0093 - acc: 0.9968 - val_loss: 0.2141 - val_acc: 0.9648\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0105 - acc: 0.9963 - val_loss: 0.2125 - val_acc: 0.9636\n",
      "Train on 12000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 3s 224us/step - loss: 0.5164 - acc: 0.8370 - val_loss: 0.2679 - val_acc: 0.9200\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.2196 - acc: 0.9305 - val_loss: 0.1763 - val_acc: 0.9481\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.1456 - acc: 0.9524 - val_loss: 0.1515 - val_acc: 0.9552\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 2s 205us/step - loss: 0.0970 - acc: 0.9688 - val_loss: 0.1403 - val_acc: 0.9597\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0757 - acc: 0.9750 - val_loss: 0.1414 - val_acc: 0.9590\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0539 - acc: 0.9827 - val_loss: 0.1577 - val_acc: 0.9590\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.1450 - val_acc: 0.9637\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 3s 243us/step - loss: 0.0325 - acc: 0.9881 - val_loss: 0.1624 - val_acc: 0.9617\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 218us/step - loss: 0.0293 - acc: 0.9893 - val_loss: 0.1627 - val_acc: 0.9637\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 2s 178us/step - loss: 0.0216 - acc: 0.9926 - val_loss: 0.1612 - val_acc: 0.9668\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0215 - acc: 0.9927 - val_loss: 0.1654 - val_acc: 0.9666\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 3s 220us/step - loss: 0.0183 - acc: 0.9937 - val_loss: 0.1715 - val_acc: 0.9667\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0128 - acc: 0.9952 - val_loss: 0.1888 - val_acc: 0.9654\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 3s 241us/step - loss: 0.0124 - acc: 0.9957 - val_loss: 0.2004 - val_acc: 0.9611\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.1843 - val_acc: 0.9676\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 3s 240us/step - loss: 0.0123 - acc: 0.9956 - val_loss: 0.1738 - val_acc: 0.9688\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 239us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.1908 - val_acc: 0.9673\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.1870 - val_acc: 0.9693\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.2172 - val_acc: 0.9652\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.0079 - acc: 0.9973 - val_loss: 0.2121 - val_acc: 0.9681\n",
      "Train on 12000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 3s 239us/step - loss: 0.5315 - acc: 0.8313 - val_loss: 0.2691 - val_acc: 0.9141\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.2095 - acc: 0.9379 - val_loss: 0.1795 - val_acc: 0.9452\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.1405 - acc: 0.9558 - val_loss: 0.1439 - val_acc: 0.9582\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 2s 170us/step - loss: 0.0993 - acc: 0.9708 - val_loss: 0.1808 - val_acc: 0.9456\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 2s 170us/step - loss: 0.0709 - acc: 0.9767 - val_loss: 0.1390 - val_acc: 0.9613\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0600 - acc: 0.9805 - val_loss: 0.1449 - val_acc: 0.9615\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 2s 182us/step - loss: 0.0435 - acc: 0.9864 - val_loss: 0.1426 - val_acc: 0.9649\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0364 - acc: 0.9883 - val_loss: 0.1487 - val_acc: 0.9650\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.0268 - acc: 0.9907 - val_loss: 0.1602 - val_acc: 0.9639\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.0234 - acc: 0.9918 - val_loss: 0.1615 - val_acc: 0.9649\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 2s 178us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.1754 - val_acc: 0.9653\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.0217 - acc: 0.9926 - val_loss: 0.1800 - val_acc: 0.9643\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.1745 - val_acc: 0.9674\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0153 - acc: 0.9952 - val_loss: 0.1763 - val_acc: 0.9680\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 2s 173us/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.2104 - val_acc: 0.9620\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 2s 180us/step - loss: 0.0131 - acc: 0.9956 - val_loss: 0.1862 - val_acc: 0.9670\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 2s 171us/step - loss: 0.0107 - acc: 0.9962 - val_loss: 0.2204 - val_acc: 0.9618\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.2068 - val_acc: 0.9669\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 172us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.1873 - val_acc: 0.9673\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 2s 170us/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.1878 - val_acc: 0.9674\n",
      "Train on 12000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.5137 - acc: 0.8394 - val_loss: 0.2999 - val_acc: 0.9104\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.1992 - acc: 0.9417 - val_loss: 0.2212 - val_acc: 0.9331\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.1356 - acc: 0.9573 - val_loss: 0.1971 - val_acc: 0.9417\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.1016 - acc: 0.9689 - val_loss: 0.2076 - val_acc: 0.9403\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.0715 - acc: 0.9774 - val_loss: 0.2113 - val_acc: 0.9435\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.0555 - acc: 0.9828 - val_loss: 0.2066 - val_acc: 0.9485\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0425 - acc: 0.9858 - val_loss: 0.1665 - val_acc: 0.9596\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 3s 220us/step - loss: 0.0333 - acc: 0.9892 - val_loss: 0.1628 - val_acc: 0.9617\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 232us/step - loss: 0.0305 - acc: 0.9892 - val_loss: 0.1586 - val_acc: 0.9654\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 3s 215us/step - loss: 0.0219 - acc: 0.9928 - val_loss: 0.1683 - val_acc: 0.9633\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 2s 204us/step - loss: 0.0193 - acc: 0.9932 - val_loss: 0.1895 - val_acc: 0.9607\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.0202 - acc: 0.9932 - val_loss: 0.1981 - val_acc: 0.9630\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 3s 239us/step - loss: 0.0157 - acc: 0.9948 - val_loss: 0.1977 - val_acc: 0.9638\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 3s 236us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.2002 - val_acc: 0.9630\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 3s 228us/step - loss: 0.0138 - acc: 0.9951 - val_loss: 0.2165 - val_acc: 0.9633\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.0101 - acc: 0.9964 - val_loss: 0.2158 - val_acc: 0.9658\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 211us/step - loss: 0.0118 - acc: 0.9963 - val_loss: 0.2214 - val_acc: 0.9628\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0118 - acc: 0.9953 - val_loss: 0.2470 - val_acc: 0.9589\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.2169 - val_acc: 0.9642\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.2454 - val_acc: 0.9617\n",
      "Train on 12000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 3s 237us/step - loss: 0.5347 - acc: 0.8283 - val_loss: 0.3143 - val_acc: 0.9030\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 3s 212us/step - loss: 0.2155 - acc: 0.9332 - val_loss: 0.2105 - val_acc: 0.9343\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.1427 - acc: 0.9569 - val_loss: 0.1768 - val_acc: 0.9463\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.1004 - acc: 0.9691 - val_loss: 0.1682 - val_acc: 0.9492\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 2s 208us/step - loss: 0.0735 - acc: 0.9775 - val_loss: 0.1760 - val_acc: 0.9499\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 3s 215us/step - loss: 0.0547 - acc: 0.9829 - val_loss: 0.1513 - val_acc: 0.9581\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 3s 215us/step - loss: 0.0434 - acc: 0.9856 - val_loss: 0.1419 - val_acc: 0.9642\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0294 - acc: 0.9905 - val_loss: 0.1605 - val_acc: 0.9623\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.0271 - acc: 0.9910 - val_loss: 0.1805 - val_acc: 0.9593\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.1635 - val_acc: 0.9643\n",
      "Epoch 11/20\n",
      "10112/12000 [========================>.....] - ETA: 0s - loss: 0.0224 - acc: 0.9931"
     ]
    }
   ],
   "source": [
    "x_folds = np.array(x_folds)\n",
    "train_validate_k(x_folds, y_folds, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3, write a few sentences below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
